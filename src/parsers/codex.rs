//! Codex CLI JSONL parser

use crate::types::{Result, ToktrackError, UsageEntry};
use chrono::{DateTime, Utc};
use serde::Deserialize;
use std::fs::File;
use std::io::{BufRead, BufReader};
use std::path::{Path, PathBuf};

use super::CLIParser;

/// Codex JSONL line types
#[derive(Deserialize)]
struct CodexJsonLine<'a> {
    #[serde(rename = "type")]
    line_type: &'a str,
    timestamp: &'a str,
    #[serde(default)]
    payload: Option<CodexPayload>,
}

#[derive(Deserialize)]
struct CodexPayload {
    #[serde(rename = "type")]
    payload_type: Option<String>,
    #[serde(default)]
    model: Option<String>,
    #[serde(default)]
    info: Option<CodexInfo>,
    #[serde(default)]
    id: Option<String>,
}

#[derive(Deserialize)]
struct CodexInfo {
    total_token_usage: Option<CodexTokenUsage>,
    #[serde(default)]
    last_token_usage: Option<CodexTokenUsage>,
}

#[derive(Deserialize, Clone)]
struct CodexTokenUsage {
    input_tokens: u64,
    output_tokens: u64,
    #[serde(default)]
    cached_input_tokens: u64,
}

/// Raw token data extracted from a token_count event
struct TokenCountData {
    timestamp: DateTime<Utc>,
    total: CodexTokenUsage,
    last: Option<CodexTokenUsage>,
}

/// Parser for Codex CLI usage data
pub struct CodexParser {
    data_dir: PathBuf,
}

impl CodexParser {
    /// Create a new parser with default data directory (~/.codex/sessions/)
    pub fn new() -> Self {
        let home = directories::BaseDirs::new()
            .map(|d| d.home_dir().to_path_buf())
            .unwrap_or_else(|| {
                eprintln!("[toktrack] Warning: Could not determine home directory");
                PathBuf::from(".")
            });
        Self {
            data_dir: home.join(".codex").join("sessions"),
        }
    }

    /// Create a parser with a custom data directory (for testing)
    #[allow(dead_code)]
    pub fn with_data_dir(data_dir: PathBuf) -> Self {
        Self { data_dir }
    }

    /// Parse a single JSONL line
    fn parse_line(&self, line: &mut [u8]) -> ParseResult {
        if line.is_empty() {
            return ParseResult::Skip;
        }

        let data: CodexJsonLine = match simd_json::from_slice(line) {
            Ok(d) => d,
            Err(_) => return ParseResult::Skip,
        };

        let payload = match &data.payload {
            Some(p) => p,
            None => return ParseResult::Skip,
        };

        if data.line_type == "turn_context" {
            if let Some(ref model) = payload.model {
                return ParseResult::Model(model.clone());
            }
            return ParseResult::Skip;
        }

        if data.line_type == "session_meta" {
            if let Some(ref id) = payload.id {
                return ParseResult::SessionId(id.clone());
            }
            return ParseResult::Skip;
        }

        if data.line_type != "event_msg" {
            return ParseResult::Skip;
        }

        let payload_type = match &payload.payload_type {
            Some(t) => t,
            None => return ParseResult::Skip,
        };

        if payload_type != "token_count" {
            return ParseResult::Skip;
        }

        let info = match &payload.info {
            Some(i) => i,
            None => return ParseResult::Skip,
        };

        let total = match &info.total_token_usage {
            Some(u) => u.clone(),
            None => return ParseResult::Skip,
        };

        let timestamp = match DateTime::parse_from_rfc3339(data.timestamp) {
            Ok(dt) => dt.with_timezone(&Utc),
            Err(_) => {
                eprintln!(
                    "[toktrack] Warning: Invalid timestamp '{}', skipping entry",
                    data.timestamp
                );
                return ParseResult::Skip;
            }
        };

        ParseResult::TokenCount(TokenCountData {
            timestamp,
            total,
            last: info.last_token_usage.clone(),
        })
    }
}

/// Result of parsing a single line
enum ParseResult {
    Skip,
    Model(String),
    SessionId(String),
    TokenCount(TokenCountData),
}

impl Default for CodexParser {
    fn default() -> Self {
        Self::new()
    }
}

impl CLIParser for CodexParser {
    fn name(&self) -> &str {
        "codex"
    }

    fn data_dir(&self) -> &Path {
        &self.data_dir
    }

    fn file_pattern(&self) -> &str {
        "**/*.jsonl"
    }

    fn parse_file(&self, path: &Path) -> Result<Vec<UsageEntry>> {
        let file = File::open(path).map_err(ToktrackError::Io)?;
        let reader = BufReader::new(file);
        let mut entries: Vec<UsageEntry> = Vec::new();
        let mut current_model: Option<String> = None;
        let mut session_id: Option<String> = None;
        let mut prev_totals = CodexTokenUsage {
            input_tokens: 0,
            output_tokens: 0,
            cached_input_tokens: 0,
        };

        for line_result in reader.lines() {
            let line = match line_result {
                Ok(l) => l,
                Err(_) => continue,
            };

            if line.is_empty() {
                continue;
            }

            let mut line_bytes = line.into_bytes();
            match self.parse_line(&mut line_bytes) {
                ParseResult::Skip => {}
                ParseResult::Model(m) => current_model = Some(m),
                ParseResult::SessionId(id) => session_id = Some(id),
                ParseResult::TokenCount(data) => {
                    // Compute delta: prefer last_token_usage, fallback to diff
                    let (delta_input, delta_output, delta_cached) =
                        if let Some(ref last) = data.last {
                            (
                                last.input_tokens,
                                last.output_tokens,
                                last.cached_input_tokens,
                            )
                        } else {
                            (
                                data.total
                                    .input_tokens
                                    .saturating_sub(prev_totals.input_tokens),
                                data.total
                                    .output_tokens
                                    .saturating_sub(prev_totals.output_tokens),
                                data.total
                                    .cached_input_tokens
                                    .saturating_sub(prev_totals.cached_input_tokens),
                            )
                        };

                    prev_totals = data.total;

                    // Skip zero-delta events
                    if delta_input == 0 && delta_output == 0 && delta_cached == 0 {
                        continue;
                    }

                    // Normalize: input_tokens = non-cached only (Claude convention)
                    let non_cached_input = delta_input.saturating_sub(delta_cached);

                    entries.push(UsageEntry {
                        timestamp: data.timestamp,
                        model: current_model.clone(),
                        input_tokens: non_cached_input,
                        output_tokens: delta_output,
                        cache_read_tokens: delta_cached,
                        cache_creation_tokens: 0,
                        thinking_tokens: 0,
                        cost_usd: None,
                        message_id: session_id.clone(),
                        request_id: None,
                        source: Some("codex".into()),
                        provider: None,
                    });
                }
            }
        }

        Ok(entries)
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::path::PathBuf;

    fn fixture_path(name: &str) -> PathBuf {
        PathBuf::from(env!("CARGO_MANIFEST_DIR"))
            .join("tests")
            .join("fixtures")
            .join("codex")
            .join(name)
    }

    #[test]
    fn test_parse_delta_sum_produces_per_turn_entries() {
        let parser = CodexParser::with_data_dir(PathBuf::from("tests/fixtures/codex"));
        let entries = parser
            .parse_file(&fixture_path("sample-session.jsonl"))
            .unwrap();

        // Two token_count events → two entries (delta per turn)
        assert_eq!(entries.len(), 2);
    }

    #[test]
    fn test_input_tokens_normalized_for_codex() {
        let parser = CodexParser::with_data_dir(PathBuf::from("tests/fixtures/codex"));
        let entries = parser
            .parse_file(&fixture_path("sample-session.jsonl"))
            .unwrap();

        // Entry 1: last_token_usage={input:150, output:75, cached:25}
        //   normalized input = 150 - 25 = 125
        let e1 = &entries[0];
        assert_eq!(e1.model, Some("o4-mini".to_string()));
        assert_eq!(e1.input_tokens, 125); // 150 - 25
        assert_eq!(e1.output_tokens, 75);
        assert_eq!(e1.cache_read_tokens, 25);

        // Entry 2: last_token_usage={input:350, output:125, cached:75}
        //   normalized input = 350 - 75 = 275
        let e2 = &entries[1];
        assert_eq!(e2.model, Some("gpt-4.1".to_string()));
        assert_eq!(e2.input_tokens, 275); // 350 - 75
        assert_eq!(e2.output_tokens, 125);
        assert_eq!(e2.cache_read_tokens, 75);
    }

    #[test]
    fn test_delta_sum_matches_expected() {
        let parser = CodexParser::with_data_dir(PathBuf::from("tests/fixtures/codex"));
        let entries = parser
            .parse_file(&fixture_path("multi-turn-session.jsonl"))
            .unwrap();

        // Turn 1: last={100,50,20} → input=80, output=50, cached=20
        // Turn 2: last={200,70,40} → input=160, output=70, cached=40
        // Turn 3: no last → diff={0,0,0} → SKIP
        // Turn 4: last={200,80,40} → input=160, output=80, cached=40
        assert_eq!(entries.len(), 3);

        assert_eq!(entries[0].input_tokens, 80);
        assert_eq!(entries[0].output_tokens, 50);
        assert_eq!(entries[0].cache_read_tokens, 20);

        assert_eq!(entries[1].input_tokens, 160);
        assert_eq!(entries[1].output_tokens, 70);
        assert_eq!(entries[1].cache_read_tokens, 40);

        assert_eq!(entries[2].input_tokens, 160);
        assert_eq!(entries[2].output_tokens, 80);
        assert_eq!(entries[2].cache_read_tokens, 40);
    }

    #[test]
    fn test_zero_delta_skipped() {
        let parser = CodexParser::with_data_dir(PathBuf::from("tests/fixtures/codex"));
        let entries = parser
            .parse_file(&fixture_path("multi-turn-session.jsonl"))
            .unwrap();

        // Turn 3 has zero delta (no last_token_usage, totals unchanged) → skipped
        // So 3 entries, not 4
        assert_eq!(entries.len(), 3);
    }

    #[test]
    fn test_skip_invalid_lines() {
        let parser = CodexParser::with_data_dir(PathBuf::from("tests/fixtures/codex"));
        let entries = parser
            .parse_file(&fixture_path("sample-session.jsonl"))
            .unwrap();

        // Invalid JSON line, null info, and non-token events are skipped
        assert_eq!(entries.len(), 2);
    }

    #[test]
    fn test_session_id_and_source() {
        let parser = CodexParser::with_data_dir(PathBuf::from("tests/fixtures/codex"));
        let entries = parser
            .parse_file(&fixture_path("sample-session.jsonl"))
            .unwrap();

        for entry in &entries {
            assert_eq!(entry.source, Some("codex".into()));
            assert_eq!(entry.message_id, Some("session-001".to_string()));
        }
    }

    #[test]
    fn test_parser_name() {
        let parser = CodexParser::new();
        assert_eq!(parser.name(), "codex");
    }

    #[test]
    fn test_parser_file_pattern() {
        let parser = CodexParser::new();
        assert_eq!(parser.file_pattern(), "**/*.jsonl");
    }

    #[test]
    fn test_parse_nonexistent_file() {
        let parser = CodexParser::new();
        let result = parser.parse_file(Path::new("/nonexistent/file.jsonl"));
        assert!(result.is_err());
    }
}
